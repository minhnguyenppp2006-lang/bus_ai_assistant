import streamlit as st
import googlemaps
import google.generativeai as genai
from datetime import datetime
import speech_recognition as sr
from gtts import gTTS
from streamlit_mic_recorder import mic_recorder
import io
import tempfile
import os

# --- Cáº¤U HÃŒNH TRANG ---
st.set_page_config(page_title="Bus AI Pro", page_icon="ğŸšŒ", layout="wide")

# --- QUáº¢N LÃ API KEY (Má»šI) ---
# Code sáº½ tá»± Ä‘á»™ng tÃ¬m key trong file secrets há»‡ thá»‘ng
try:
    GOOGLE_MAPS_KEY = st.secrets["GOOGLE_MAPS_API_KEY"]
    GEMINI_API_KEY = st.secrets["GEMINI_API_KEY"]
except FileNotFoundError:
    st.error("âš ï¸ Lá»—i cáº¥u hÃ¬nh: ChÆ°a tÃ¬m tháº¥y file secrets.toml (Náº¿u cháº¡y local) hoáº·c Secrets (Náº¿u cháº¡y trÃªn Cloud).")
    st.stop()
except KeyError:
    st.error("âš ï¸ Lá»—i cáº¥u hÃ¬nh: Thiáº¿u API Key trong file secrets.")
    st.stop()

# --- SIDEBAR (Chá»‰ cÃ²n cÃ¡c tÃ¹y chá»n cho User) ---
with st.sidebar:
    st.header("âš™ï¸ TÃ¹y chá»n")
    auto_speak = st.checkbox("Tá»± Ä‘á»™ng Ä‘á»c (TTS)", value=True)
    st.divider()
    st.header("ğŸ¯ TiÃªu chÃ­ tá»‘i Æ°u")
    optimize_mode = st.radio("Æ¯u tiÃªn:", ["Thá»i gian ngáº¯n nháº¥t", "Ãt Ä‘i bá»™ nháº¥t", "Ãt chuyá»ƒn tuyáº¿n nháº¥t"])

# --- CÃC HÃ€M LOGIC (GIá»® NGUYÃŠN) ---

def get_routes(start, end, api_key):
    # Logic cÅ© nhÆ°ng dÃ¹ng api_key Ä‘Æ°á»£c truyá»n vÃ o tá»« secrets
    if not api_key: return "Thiáº¿u API Key"
    gmaps = googlemaps.Client(key=api_key)
    now = datetime.now()
    try:
        directions_result = gmaps.directions(
            start, end, mode="transit", transit_mode="bus", departure_time=now, alternatives=True, language="vi"
        )
        return directions_result
    except Exception as e:
        return f"Lá»—i: {str(e)}"

def analyze_routes(routes_data, mode):
    # (Giá»¯ nguyÃªn logic phÃ¢n tÃ­ch nhÆ° bÃ i trÆ°á»›c)
    if not routes_data or isinstance(routes_data, str): return []
    processed_routes = []
    for route in routes_data:
        leg = route['legs'][0]
        duration_value = leg['duration']['value']
        walking_distance = 0
        transfers = 0
        bus_names = []
        next_bus_time = 0
        
        for step in leg['steps']:
            if step['travel_mode'] == 'WALKING': walking_distance += step['distance']['value']
            elif step['travel_mode'] == 'TRANSIT':
                transfers += 1
                bus_names.append(step['transit_details']['line'].get('short_name', 'Bus'))
                if next_bus_time == 0: # Láº¥y cháº·ng bus Ä‘áº§u
                    dep = step['transit_details']['departure_time']['value']
                    next_bus_time = max(0, int((datetime.fromtimestamp(dep) - datetime.now()).total_seconds() / 60))

        processed_routes.append({
            "summary": f"Xe {', '.join(bus_names)}",
            "duration_text": leg['duration']['text'],
            "duration_val": duration_value,
            "walking_text": f"{walking_distance}m Ä‘i bá»™",
            "walking_val": walking_distance,
            "transfers": transfers,
            "wait_time": next_bus_time,
            "raw_steps": leg['steps']
        })

    if mode == "Thá»i gian ngáº¯n nháº¥t": processed_routes.sort(key=lambda x: x['duration_val'])
    elif mode == "Ãt Ä‘i bá»™ nháº¥t": processed_routes.sort(key=lambda x: x['walking_val'])
    elif mode == "Ãt chuyá»ƒn tuyáº¿n nháº¥t": processed_routes.sort(key=lambda x: x['transfers'])
    return processed_routes

def text_to_speech(text):
    try:
        tts = gTTS(text=text, lang='vi')
        fp = io.BytesIO()
        tts.write_to_fp(fp)
        return fp
    except: return None

def process_audio(audio_bytes):
    r = sr.Recognizer()
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
            tmp.write(audio_bytes)
            name = tmp.name
        with sr.AudioFile(name) as src:
            audio = r.record(src)
            text = r.recognize_google(audio, language="vi-VN")
        os.remove(name)
        return text
    except: return None

# --- GIAO DIá»†N CHÃNH ---

st.title("ğŸšŒ Bus Assistant (Public Version)")

# Khá»Ÿi táº¡o Gemini tá»« Secrets
genai.configure(api_key=GEMINI_API_KEY)
model = genai.GenerativeModel('gemini-2.5-pro')

col1, col2 = st.columns([1.2, 0.8])

with col1:
    with st.form("search_form"):
        c1, c2 = st.columns(2)
        origin = c1.text_input("Äiá»ƒm Ä‘i")
        destination = c2.text_input("Äiá»ƒm Ä‘áº¿n")
        submitted = st.form_submit_button("TÃ¬m Ä‘Æ°á»ng ğŸš€")

    if submitted and origin and destination:
        with st.spinner("Äang xá»­ lÃ½..."):
            # Gá»i hÃ m vá»›i KEY láº¥y tá»« secrets
            raw_data = get_routes(origin, destination, GOOGLE_MAPS_KEY)
            
            if isinstance(raw_data, str) and "Lá»—i" in raw_data:
                st.error(f"Há»‡ thá»‘ng Ä‘ang báº£o trÃ¬ hoáº·c quÃ¡ táº£i. ({raw_data})")
            elif raw_data:
                routes = analyze_routes(raw_data, optimize_mode)
                best = routes[0]
                
                st.success(f"NÃªn Ä‘i: {best['summary']}")
                st.metric("Thá»i gian chá» xe", f"{best['wait_time']} phÃºt")
                
                context = f"Lá»™ trÃ¬nh: {best['summary']}, háº¿t {best['duration_text']}. Äi bá»™ {best['walking_text']}."
                st.session_state['route_context'] = context
                
                if auto_speak:
                    aud = text_to_speech(f"HÃ£y Ä‘Ã³n {best['summary']}. Xe Ä‘áº¿n trong {best['wait_time']} phÃºt.")
                    if aud: st.audio(aud, format='audio/mp3', start_time=0)
            else:
                st.warning("KhÃ´ng tÃ¬m tháº¥y tuyáº¿n xe nÃ o.")

with col2:
    st.subheader("ğŸ’¬ Trá»£ lÃ½ áº£o")
    chat_box = st.container(height=400)
    
    if "messages" not in st.session_state: st.session_state.messages = []
    
    with chat_box:
        for m in st.session_state.messages: st.chat_message(m["role"]).write(m["content"])
        
    text_in = st.chat_input("Há»i tÃ´i...")
    mic_in = mic_recorder(start_prompt="ğŸ¤", stop_prompt="â¹ï¸", key='mic')
    
    final_in = text_in
    if mic_in and ('last_audio' not in st.session_state or st.session_state.last_audio != mic_in['id']):
        st.session_state.last_audio = mic_in['id']
        t = process_audio(mic_in['audio']['bytes'])
        if t: final_in = t
        
    if final_in:
        st.session_state.messages.append({"role":"user", "content":final_in})
        st.chat_message("user").write(final_in)
        
        ctx = st.session_state.get('route_context', '')
        # Prompt Ä‘Æ¡n giáº£n hÃ³a Ä‘á»ƒ tiáº¿t kiá»‡m token
        res = model.generate_content(f"Context: {ctx}. User: {final_in}. Answer short in Vietnamese.").text
        
        st.session_state.messages.append({"role":"assistant", "content":res})
        st.chat_message("assistant").write(res)
        if auto_speak:
            a = text_to_speech(res)

            if a: st.audio(a, format='audio/mp3')
